{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIE1512H Project\n",
    "\n",
    "### Topic: Predicting Station-Level Demand in Bike-Sharing Systems\n",
    "\n",
    "Name: Tanya Tang<br>\n",
    "Version: V1<br>\n",
    "Date: March 22, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the paper this project is based on is to improve the effectiveness of station-level rebalancing activites within bike-sharing systems by obtaining more accurate minimum inventory levels per station. \n",
    "\n",
    "The authors used historical bike trip data along with hourly weather data to train several machine learning models. The model features fell within two categories, time-related features from the bike data (i.e. year, month, etc.) and weather-related features (i.e. precipitation, temperature, etc.). To reduce overfitting, the data is first transformed onto a reduced space using four different reduction techniques:\n",
    "1. No reduction, baseline\n",
    "2. Group all stations into one cluster\n",
    "3. Kmeans - cluster stations into k clusters\n",
    "4. Singular value decomposition\n",
    "\n",
    "After reducing the problem, four different prediction methods were tested:\n",
    "1. Linear regression\n",
    "2. Multi-layer perceptron\n",
    "3. Gradient boosted tree\n",
    "4. Random forest\n",
    "\n",
    "Once a prediction was made on the reduced problem, the reduction needed to be inverted to obtain the full problem predictions. Each reduction technique has its own specific inversion process. \n",
    "\n",
    "The solution techniques were tested and scored based on the performance of its generated minimum station inventory levels. \n",
    "\n",
    "In this project, a similar methodology will be followed, with modifications and additions on the feature construction aspect. Focusing on the city of San Francisco, historical bike data and weather data will be used in conjunction with crime data. Features from all three data sets will be constructed, and several prediction techniques will be tested. As will be further explained in later sections, the reduction/inversion step was omitted from this project due to its more theoretical nature. Thus, we tested three of the four prediction methods from the paper, linear regression, gradient-boosted tree, and random forest. As per feedback from V1, before working on the full dataset, this notebook first applies the three prediction methods we tested to the toy dataset prepared from V1. Then, we will prepare the full dataset in the same manner as V1 and test the three prediction models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Project Versions/Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Data Understanding (collecting, describing, exploring, verifying)\n",
    "* Data Preparation on Test Scale (selecting, cleaning, constructing, integrating, formatting)\n",
    "\n",
    "Actual Timeline:\n",
    "* Looking up potential datasets and collecting from online sources/deciding which datasets to use (0.5 hr)\n",
    "* Exploring entries/schema of each chosen dataset to understand discrepancies and connections between datasets (1 hr)\n",
    "* Loading/cleaning raw data within each dataset, taking a closer look at which entries are relevant (2 hr)\n",
    "* Calculating connections between different datasets and joining tables to eventually form a single aggregated table containing all relevant information for a single entry (4 hrs)\n",
    "* Verifying data preparation steps (i.e. tables joined correctly, entries were not lost throughout the preparation process, no null values exist) (1 hr)\n",
    "\n",
    "#### V2\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Data Preparation on Full Scale (selecting, cleaning, constructing, integrating, formatting)\n",
    "* Modeling (selecting techniques, generating test design, building, assessing)\n",
    "\n",
    "Planned Timeline:\n",
    "* Recreate data preparation tasks from V1 on full dataset (1 hr)\n",
    "* Research reduction/prediction techniques and how to apply them using python packages (1 hr)\n",
    "* *Implement reduction techniques (4 hr) (did not complete)*\n",
    "* Implement regression techniques (4 hr)\n",
    "* *Implement inversion techniques (4 hr) (did not complete)*\n",
    "* Test implemented models (1 hr)\n",
    "\n",
    "Actual Timeline:\n",
    "* Research prediction techniques and how to apply them using python package (0.5 hr)\n",
    "* Apply ML models to dataset prepared in V1 (should have been included in V1) (3 hr)\n",
    "* Recreate data preparation tasks from V1 on full dataset (2 hr)\n",
    "* Implement regression techniques (3 hr)\n",
    "* Apply implemented models to test data to make predictions (4 hr)\n",
    "    * NOTE: Linear regression on all stations takes ~10 min to complete, gradient-boosted tree on all stations takes ~1 hr to complete, random forest on all stations takes ~30 min to complete\n",
    "\n",
    "#### V3\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Modeling (assessing)\n",
    "* Evaluation (evaluting results, reviewing process, determining next steps)\n",
    "\n",
    "Planned Timeline (Week 3: March 26 to April 2):\n",
    "* Transform predictions into inventory model for test data time range (4 hr)\n",
    "* Create visualizations and compare performance between models (3 hr)\n",
    "* Review project activites to verify correctness (1 hr)\n",
    "* Summarize project and list some potential next steps (1 hr)\n",
    "\n",
    "#### F\n",
    "\n",
    "Planned Timeline (Week 4: April 2 to April 9):\n",
    "* Write report (6 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Date Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation will be completed on data from January 2018 to February 2020 in the same manner as was completed for the smaller dataset in V1. Following a similar partitioning as the paper, training will be done on data from January 2018 to July 2019, validation will be done on data from August to Octoher 2019, and testing will be done on data from November 2019 to January 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "* The linear regression model will be chosen from 6 different models via cross validations, the three regularization parameters are chosen from $(0.3, 0,5, 0.7)$ and the two elastic net parameters are chosen from $(0.5, 0.8)$\n",
    "* These parameters were selected by taking the default value given by spark, and slightly varying that value\n",
    "* The models will be evaluated by the built in RegressionEvaluator class in spark, giving an $R^2$ value\n",
    "\n",
    "#### Gradient-Boosted Tree\n",
    "* The gradient-boosted tree model will be chosen from 2 different models via cross validations, the two maximum depth parameters are chosen from $(3, 5)$\n",
    "* These parameters were selected by taking the value from the paper (5) and giving a lower parameter option as we have not applied any reduction methods so our model is more prone to overfitting\n",
    "* The models will be evaluated by the built in RegressionEvaluator class in spark, giving an $R^2$ value\n",
    "\n",
    "#### Random Forest\n",
    "* The random forest model will be chosen from 2 different models via cross validations, the two forest size (number of trees) parameters are chosen from $(50, 100)$\n",
    "* These parameters were selected by taking the value from the paper (100), and giving a lower parameter option as we have not applied any reduction methods so our model is more prone to overgitting\n",
    "* The models will be evaluated by the built in RegressionEvaluator class in spark, giving an $R^2$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to make sure we can apply all of our modelling techniques using MLlib to the toy dataset consisting of two months of data prepared in V1. We will now reload and prepare the data in the same manner as V1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Java locations\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/\"\n",
    "os.environ[\"JRE_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/\"\n",
    "\n",
    "# Initialize spark\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/Cellar/apache-spark@2.3.2/2.3.2/libexec/\")\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"appName\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from previously cleaned **aggregateTripWeatherCrimeData** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1data = spark.read.parquet('resources/v1/v1_aggregate_trip_weather_crime_data.parquet')\n",
    "v1data.createOrReplaceTempView('v1data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample loaded data to make sure it's correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>departure_count</th>\n",
       "      <th>arrival_count</th>\n",
       "      <th>wind_speed_rate</th>\n",
       "      <th>visibility</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>num_incidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>10th Ave at E 15th St</td>\n",
       "      <td>37.792714</td>\n",
       "      <td>-122.248780</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>10th Ave at E 15th St</td>\n",
       "      <td>37.792714</td>\n",
       "      <td>-122.248780</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>10th Ave at E 15th St</td>\n",
       "      <td>37.792714</td>\n",
       "      <td>-122.248780</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3219.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8047.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>10th St at Fallon St</td>\n",
       "      <td>37.797673</td>\n",
       "      <td>-122.262997</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>11th St at Bryant St</td>\n",
       "      <td>37.770030</td>\n",
       "      <td>-122.411726</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16093.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  day  hour           station_name        lat        long  \\\n",
       "0      1   26    18  10th Ave at E 15th St  37.792714 -122.248780   \n",
       "1      2    5    17  10th Ave at E 15th St  37.792714 -122.248780   \n",
       "2      2   11    16  10th Ave at E 15th St  37.792714 -122.248780   \n",
       "3      1    8    18   10th St at Fallon St  37.797673 -122.262997   \n",
       "4      1   17    21   10th St at Fallon St  37.797673 -122.262997   \n",
       "5      1   20     0   10th St at Fallon St  37.797673 -122.262997   \n",
       "6      1   24    10   10th St at Fallon St  37.797673 -122.262997   \n",
       "7      1   29    23   10th St at Fallon St  37.797673 -122.262997   \n",
       "8      2   20    14   10th St at Fallon St  37.797673 -122.262997   \n",
       "9      1    2    11   11th St at Bryant St  37.770030 -122.411726   \n",
       "\n",
       "   departure_count  arrival_count  wind_speed_rate  visibility  temperature  \\\n",
       "0                0              1              0.0     16093.0         10.0   \n",
       "1                2              1              0.0     16093.0         14.4   \n",
       "2                0              1             46.0     16093.0          9.4   \n",
       "3                1              0             15.0      3219.0         11.1   \n",
       "4                0              1             31.0      8047.0         13.9   \n",
       "5                1              0             67.0     16093.0         12.2   \n",
       "6                0              2              0.0     16093.0          7.8   \n",
       "7                1              0             21.0     16093.0         13.9   \n",
       "8                1              3             15.0     16093.0          2.2   \n",
       "9                1              1              0.0     16093.0         10.0   \n",
       "\n",
       "   precipitation  num_incidents  \n",
       "0            0.0              0  \n",
       "1            0.0              0  \n",
       "2            0.0              0  \n",
       "3           18.0              0  \n",
       "4            0.0              0  \n",
       "5            0.0              0  \n",
       "6            0.0              0  \n",
       "7            0.0              0  \n",
       "8            0.0              0  \n",
       "9            0.0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM v1data\n",
    "LIMIT 10\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses four reduction techniques to transform the data from its original dimension to a reduced dimension. After further investigation, we will not be pursuing these reduction method because the reduction and reconstruction techniques presented in the paper are fairly theoretical. Thus, we will now apply the prediction techniques. The first step to applying any prediction model is to transform the dataset into the appropriate format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two additional time features we need to add to this data, weekday and holiday indicators. Those indicators will be categorical, 0 if the day is a weekday or a holiday, or 1 if it is not. This data will be loaded in from a csv file and joined with the table above. We can also prune the lat/long columns as they will not be used as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeData = spark.read.csv('resources/v1/v1_time.csv',\n",
    "                          header=True,\n",
    "                          inferSchema=True)\n",
    "timeData.createOrReplaceTempView('temp')\n",
    "spark.sql(\"\"\"\n",
    "SELECT MONTH(TO_TIMESTAMP(date, \"yyyy/mm/dd\")) AS month, DAY(TO_TIMESTAMP(date, \"yyyy/mm/dd\")) AS day, weekday, holiday\n",
    "FROM temp\n",
    "\"\"\").createOrReplaceTempView('timeData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "|month|day|hour|        station_name|departure_count|arrival_count|wind_speed_rate|visibility|temperature|precipitation|num_incidents|weekday|holiday|\n",
      "+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "|    1| 26|  18|10th Ave at E 15t...|              0|            1|            0.0|   16093.0|       10.0|          0.0|            0|      0|      0|\n",
      "|    2|  5|  17|10th Ave at E 15t...|              2|            1|            0.0|   16093.0|       14.4|          0.0|            0|      1|      0|\n",
      "|    2| 11|  16|10th Ave at E 15t...|              0|            1|           46.0|   16093.0|        9.4|          0.0|            0|      1|      0|\n",
      "|    1|  8|  18|10th St at Fallon St|              1|            0|           15.0|    3219.0|       11.1|         18.0|            0|      1|      0|\n",
      "|    1| 17|  21|10th St at Fallon St|              0|            1|           31.0|    8047.0|       13.9|          0.0|            0|      1|      0|\n",
      "|    1| 20|   0|10th St at Fallon St|              1|            0|           67.0|   16093.0|       12.2|          0.0|            0|      0|      0|\n",
      "|    1| 24|  10|10th St at Fallon St|              0|            2|            0.0|   16093.0|        7.8|          0.0|            0|      1|      0|\n",
      "|    1| 29|  23|10th St at Fallon St|              1|            0|           21.0|   16093.0|       13.9|          0.0|            0|      1|      0|\n",
      "|    2| 20|  14|10th St at Fallon St|              1|            3|           15.0|   16093.0|        2.2|          0.0|            0|      1|      0|\n",
      "|    1|  2|  11|11th St at Bryant St|              1|            1|            0.0|   16093.0|       10.0|          0.0|            0|      1|      0|\n",
      "|    1| 22|  18|11th St at Bryant St|              6|            2|           57.0|   16093.0|       13.3|          0.0|            1|      1|      0|\n",
      "|    1| 28|  20|11th St at Bryant St|              3|            0|           26.0|   16093.0|       15.0|          0.0|            0|      1|      0|\n",
      "|    2|  9|  14|11th St at Bryant St|              1|            2|            0.0|   16093.0|       11.1|          0.0|            0|      0|      0|\n",
      "|    2| 16|  17|11th St at Bryant St|              3|            0|            0.0|   16093.0|       11.1|          0.0|            1|      0|      0|\n",
      "|    2| 20|  17|11th St at Bryant St|              4|            0|           36.0|   16093.0|        7.8|          0.0|            1|      1|      0|\n",
      "|    2| 28|  23|11th St at Bryant St|              1|            0|           46.0|   16093.0|       12.2|          0.0|            1|      1|      0|\n",
      "|    1| 29|  13|11th St at Natoma St|              0|            1|           15.0|   16093.0|        9.4|          0.0|            1|      1|      0|\n",
      "|    1|  8|  16|  12th St at 4th Ave|              0|            1|           15.0|    3219.0|       10.6|         36.0|            0|      1|      0|\n",
      "|    1| 13|   9|  12th St at 4th Ave|              1|            0|            0.0|   12875.0|       10.0|          0.0|            0|      0|      0|\n",
      "|    2|  6|   7|  12th St at 4th Ave|              2|            0|            0.0|   16093.0|       13.9|          0.0|            0|      1|      0|\n",
      "+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlData = spark.sql(\"\"\"\n",
    "SELECT a.month, a.day, hour, station_name, departure_count, arrival_count, wind_speed_rate, visibility, temperature, precipitation, num_incidents, weekday, holiday FROM\n",
    "v1data AS a\n",
    "JOIN\n",
    "timeData AS b\n",
    "ON\n",
    "a.month == b.month\n",
    "AND a.day == b.day\n",
    "\"\"\")\n",
    "mlData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the first prediction model, linear regression, to each station by partitioning the dataset by station. For these initial application tests on the V1 dataset, we will not be tuning any hyperparameters, but when we model the full dataset, we will create cross validation models for model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 value on predicting departure counts for test data on station Row(station_name='19th St at Florida St'):  0.026830841201329392\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='19th St at Florida St'):  -0.0012861847934118043\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='5th St at Brannan St'):  0.08627169183408467\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='5th St at Brannan St'):  0.04445117794212072\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.01602284041674107\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.0026434217235493662\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='Townsend St at 7th St'):  -1.480573444466608\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='Townsend St at 7th St'):  0.05445759274965423\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='San Salvador St at 1st St'):  -0.003474397102719351\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='San Salvador St at 1st St'):  -0.0002924293797799127\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Get station names\n",
    "station_names = spark.sql(\"\"\"\n",
    "SELECT DISTINCT station_name\n",
    "FROM v1data\n",
    "\"\"\")\n",
    "\n",
    "# Predict departure and arrival counts\n",
    "for row in station_names.rdd.collect()[0:5]:\n",
    "    \n",
    "    # Assemble features into a single vector\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                        'day',\n",
    "                                        'hour',\n",
    "                                        'wind_speed_rate',\n",
    "                                        'visibility',\n",
    "                                        'temperature',\n",
    "                                        'precipitation',\n",
    "                                        'num_incidents',\n",
    "                                        'weekday',\n",
    "                                        'holiday'],\n",
    "                              outputCol='features')\n",
    "    filteredStation = mlData.filter(mlData.station_name == row.__getattr__('station_name'))\n",
    "    stationDF = vectorA.transform(filteredStation)\n",
    "    stationDF = stationDF.select('features', 'departure_count', 'arrival_count')\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    splits = stationDF.randomSplit([0.7, 0.3])\n",
    "    trainDF = splits[0]\n",
    "    testDF = splits[1]\n",
    "    \n",
    "    # Train model and evaluate on test set (departure count)\n",
    "    lr = LinearRegression(featuresCol='features',\n",
    "                          labelCol='departure_count',\n",
    "                          maxIter=100,\n",
    "                          regParam=0.3,\n",
    "                          elasticNetParam=0.8)\n",
    "    lr_model = lr.fit(trainDF)\n",
    "    lr_pred = lr_model.transform(testDF)\n",
    "    lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                      labelCol='departure_count',\n",
    "                                      metricName='r2')\n",
    "    print(f'R^2 value on predicting departure counts for test data on station {row}: ', lr_evaluator.evaluate(lr_pred))\n",
    "    \n",
    "    # Train model and evaluate on test set (arrival count)\n",
    "    lr = LinearRegression(featuresCol='features',\n",
    "                          labelCol='arrival_count',\n",
    "                          maxIter=100,\n",
    "                          regParam=0.3,\n",
    "                          elasticNetParam=0.8)\n",
    "    lr_model = lr.fit(trainDF)\n",
    "    lr_pred = lr_model.transform(testDF)\n",
    "    lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                      labelCol='arrival_count',\n",
    "                                      metricName='r2')\n",
    "    print(f'R^2 value on predicting arrival counts for test data on station {row}: ', lr_evaluator.evaluate(lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next prediction method explored in the paper is a multi-layer perceptron model. Spark's ML library only appears to contain a multi-layer perceptron classifier model. Thus, we will not be able to implement this model as this project is dealing with a regression problem. Thus, we will skip to the third prediction method, a gradient-boosted tree. We will follow the same steps as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 value on predicting departure counts for test data on station Row(station_name='19th St at Florida St'):  -0.18645470031943523\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='19th St at Florida St'):  -0.42694145099486236\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='5th St at Brannan St'):  -0.10472018786287962\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='5th St at Brannan St'):  0.3847710896270803\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.3035704844680538\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.14558693311275883\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='Townsend St at 7th St'):  0.31309933694012726\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='Townsend St at 7th St'):  0.30966988069958434\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='San Salvador St at 1st St'):  -0.04822470158048486\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='San Salvador St at 1st St'):  -1.7510720236109063\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Predict departure and arrival counts\n",
    "for row in station_names.rdd.collect()[0:5]:\n",
    "    \n",
    "    # Assemble features into a single vector\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                        'day',\n",
    "                                        'hour',\n",
    "                                        'wind_speed_rate',\n",
    "                                        'visibility',\n",
    "                                        'temperature',\n",
    "                                        'precipitation',\n",
    "                                        'num_incidents',\n",
    "                                        'weekday',\n",
    "                                        'holiday'],\n",
    "                              outputCol='features')\n",
    "    filteredStation = mlData.filter(mlData.station_name == row.__getattr__('station_name'))\n",
    "    stationDF = vectorA.transform(filteredStation)\n",
    "    stationDF = stationDF.select('features', 'departure_count', 'arrival_count')\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    splits = stationDF.randomSplit([0.7, 0.3])\n",
    "    trainDF = splits[0]\n",
    "    testDF = splits[1]\n",
    "    \n",
    "    # Train model and evaluate on test set (departure count)\n",
    "    gbtr = GBTRegressor(labelCol='departure_count',\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10)\n",
    "    gbtr_model = gbtr.fit(trainDF)\n",
    "    gbtr_pred = gbtr_model.transform(testDF)\n",
    "    gbtr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                         labelCol='departure_count',\n",
    "                                         metricName='r2')\n",
    "    print(f'R^2 value on predicting departure counts for test data on station {row}: ',\n",
    "          gbtr_evaluator.evaluate(gbtr_pred))\n",
    "    \n",
    "    # Train model and evaluate on test set (arrival count)\n",
    "    gbtr = GBTRegressor(labelCol='arrival_count',\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10)\n",
    "    gbtr_model = gbtr.fit(trainDF)\n",
    "    gbtr_pred = gbtr_model.transform(testDF)\n",
    "    gbtr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                         labelCol='arrival_count',\n",
    "                                         metricName='r2')\n",
    "    print(f'R^2 value on predicting arrival counts for test data on station {row}: ',\n",
    "          gbtr_evaluator.evaluate(gbtr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last prediction method is a random forest model. Again, we will follow the same steps as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 value on predicting departure counts for test data on station Row(station_name='19th St at Florida St'):  0.07628345845233486\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='19th St at Florida St'):  -0.11294351616097265\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='5th St at Brannan St'):  0.06837177822727059\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='5th St at Brannan St'):  0.32911018225041266\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.28320310797311876\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='College Ave at Harwood Ave'):  -0.49946007785063795\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='Townsend St at 7th St'):  0.4926272783836336\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='Townsend St at 7th St'):  0.5410021980638576\n",
      "R^2 value on predicting departure counts for test data on station Row(station_name='San Salvador St at 1st St'):  -0.3184998572602016\n",
      "R^2 value on predicting arrival counts for test data on station Row(station_name='San Salvador St at 1st St'):  -1.060681903263725\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Predict departure and arrival counts\n",
    "for row in station_names.rdd.collect()[0:5]:\n",
    "    \n",
    "    # Assemble features into a single vector\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                        'day',\n",
    "                                        'hour',\n",
    "                                        'wind_speed_rate',\n",
    "                                        'visibility',\n",
    "                                        'temperature',\n",
    "                                        'precipitation',\n",
    "                                        'num_incidents',\n",
    "                                        'weekday',\n",
    "                                        'holiday'],\n",
    "                              outputCol='features')\n",
    "    filteredStation = mlData.filter(mlData.station_name == row.__getattr__('station_name'))\n",
    "    stationDF = vectorA.transform(filteredStation)\n",
    "    stationDF = stationDF.select('features', 'departure_count', 'arrival_count')\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    splits = stationDF.randomSplit([0.7, 0.3])\n",
    "    trainDF = splits[0]\n",
    "    testDF = splits[1]\n",
    "    \n",
    "    # Train model and evaluate on test set (departure count)\n",
    "    rfr = GBTRegressor(labelCol='departure_count',\n",
    "                       featuresCol=\"features\")\n",
    "    rfr_model = rfr.fit(trainDF)\n",
    "    rfr_pred = rfr_model.transform(testDF)\n",
    "    rfr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                        labelCol='departure_count',\n",
    "                                        metricName='r2')\n",
    "    print(f'R^2 value on predicting departure counts for test data on station {row}: ',\n",
    "          rfr_evaluator.evaluate(rfr_pred))\n",
    "    \n",
    "    # Train model and evaluate on test set (arrival count)\n",
    "    rfr = GBTRegressor(labelCol='arrival_count',\n",
    "                       featuresCol=\"features\")\n",
    "    rfr_model = rfr.fit(trainDF)\n",
    "    rfr_pred = rfr_model.transform(testDF)\n",
    "    rfr_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                        labelCol='arrival_count',\n",
    "                                        metricName='r2')\n",
    "    print(f'R^2 value on predicting arrival counts for test data on station {row}: ',\n",
    "          rfr_evaluator.evaluate(rfr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have applied three prediction methods to the toy dataset, so we can move onto applying the entire data preparation/modelling framework developed in V1 and above to the full dataset. First, let's load all of the trip/crime/weather data from the beginning of January 2018 to the end of January 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration_sec: integer (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- bike_share_for_all_trip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in ['2018', '2019', '2020']:\n",
    "    for j in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        if i == '2020' and j == '02':\n",
    "            break\n",
    "        filepath = f'resources/raw_data/{i}{j}-fordgobike-tripdata.csv'\n",
    "        month_data = spark.read.csv(filepath,\n",
    "                                    header=True,\n",
    "                                    inferSchema=True,\n",
    "                                    sep=',',\n",
    "                                    mode='DROPMALFORMED')\n",
    "        data.append(month_data)\n",
    "for i in range(1, len(data)):\n",
    "    data[0] = data[0].union(data[i])\n",
    "data[0].createOrReplaceTempView('temp')\n",
    "data[0].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM\n",
    "(SELECT CAST(start_time AS timestamp) AS start_time, CAST(end_time AS timestamp), start_station_name, start_station_latitude, start_station_longitude, end_station_name, end_station_latitude, end_station_longitude\n",
    "FROM temp)\n",
    "WHERE start_time IS NOT NULL\n",
    "AND end_time IS NOT NULL\n",
    "AND start_station_name IS NOT NULL\n",
    "AND end_station_name IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView('tripData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- wind_speed_rate: integer (nullable = true)\n",
      " |-- visibility: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- precipitation: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "data = []\n",
    "for i in ['2018', '2019', '2020']:\n",
    "    filepath = f'resources/raw_data/sf_weather_{i}.csv'\n",
    "    data.append(spark.read.csv(filepath,\n",
    "                               header=True,\n",
    "                               inferSchema=True,\n",
    "                               sep=',',\n",
    "                               mode='DROPMALFORMED'))\n",
    "for i in range(1, len(data)):\n",
    "    data[0] = data[0].union(data[i])\n",
    "data[0].createOrReplaceTempView('temp')\n",
    "weather_data = spark.sql(\"\"\"\n",
    "SELECT (DATE + INTERVAL '4' MINUTE) AS timestamp,\n",
    "LATITUDE AS lat,\n",
    "LONGITUDE AS long,\n",
    "WND AS wind_speed_rate,\n",
    "VIS AS visibility,\n",
    "TMP AS temperature,\n",
    "AA1 AS precipitation\n",
    "FROM temp\n",
    "WHERE SOURCE = 7 AND REPORT_TYPE = \"FM-15\"\n",
    "\"\"\")\n",
    "weather_data = weather_data.withColumn('wind_speed_rate', split('wind_speed_rate', '\\,')[3])\n",
    "weather_data = weather_data.withColumn('wind_speed_rate', weather_data['wind_speed_rate'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('visibility', split('visibility', '\\,')[0])\n",
    "weather_data = weather_data.withColumn('visibility', weather_data['visibility'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('temperature', split('temperature', '\\,')[0])\n",
    "weather_data = weather_data.withColumn('temperature', weather_data['temperature'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('temperature', weather_data['temperature'] / 10)\n",
    "weather_data = weather_data.withColumn('precipitation', split('precipitation', '\\,')[1])\n",
    "weather_data = weather_data.withColumn('precipitation', weather_data['precipitation'].cast(IntegerType()))\n",
    "weather_data.createOrReplaceTempView('weatherData')\n",
    "weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- incident_timestamp: timestamp (nullable = true)\n",
      " |-- incident_id: integer (nullable = true)\n",
      " |-- report_code: string (nullable = true)\n",
      " |-- incident_code: integer (nullable = true)\n",
      " |-- incident_category: string (nullable = true)\n",
      " |-- incident_subcategory: string (nullable = true)\n",
      " |-- resolution: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'resources/raw_data/sfpd_crime.csv'\n",
    "crime_data = spark.read.csv(filepath,\n",
    "                            header=True,\n",
    "                            inferSchema=True,\n",
    "                            sep=',',\n",
    "                            mode='DROPMALFORMED')\n",
    "crime_data.createOrReplaceTempView('temp')\n",
    "crime_data = spark.sql(\"\"\"\n",
    "SELECT TO_TIMESTAMP(`Incident Datetime`, \"yyyy/MM/dd hh:mm\") AS incident_timestamp,\n",
    "`Incident ID` AS incident_id,\n",
    "`Report Type Code` AS report_code,\n",
    "`Incident Code` AS incident_code,\n",
    "`Incident Category` AS incident_category,\n",
    "`Incident Subcategory` AS incident_subcategory,\n",
    "`Resolution` AS resolution,\n",
    "`Latitude` as lat,\n",
    "`Longitude` as long\n",
    "FROM temp\n",
    "WHERE `Latitude` IS NOT NULL\n",
    "AND `Incident Category` IS NOT NULL\n",
    "\"\"\")\n",
    "crime_data.createOrReplaceTempView('crimeData')\n",
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's aggregate the loaded data into a single table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = spark.sql(\"\"\"\n",
    "SELECT\n",
    "COALESCE(departure_count, 0) AS departure_count,\n",
    "COALESCE(arrival_count, 0) AS arrival_count,\n",
    "COALESCE(departure.year, arrival.year) AS year,\n",
    "COALESCE(departure.hour, arrival.hour) AS hour,\n",
    "COALESCE(departure.day, arrival.day) AS day,\n",
    "COALESCE(departure.month, arrival.month) AS month,\n",
    "COALESCE(departure.start_station_name, arrival.end_station_name) AS station_name,\n",
    "COALESCE(departure.lat, arrival.lat) AS lat,\n",
    "COALESCE(departure.long, arrival.long) AS long\n",
    "FROM\n",
    "    (SELECT COUNT(1) AS departure_count, hour, day, month, year, start_station_name, lat, long\n",
    "    FROM\n",
    "        (\n",
    "        SELECT YEAR(start_time) AS year, MONTH(start_time) AS month, DAY(start_time) AS day, HOUR(start_time) AS hour, start_station_name, start_station_latitude AS lat, start_station_longitude AS long\n",
    "        FROM tripData\n",
    "        )\n",
    "    GROUP BY start_station_name, lat, long, hour, day, month, year\n",
    "    ORDER BY year, month, day, hour, start_station_name) AS departure\n",
    "FULL OUTER JOIN\n",
    "    (SELECT COUNT(1) AS arrival_count, hour, day, month, year, end_station_name, lat, long\n",
    "    FROM\n",
    "        (\n",
    "        SELECT YEAR(end_time) AS year, MONTH(end_time) AS month, DAY(end_time) AS day, HOUR(end_time) AS hour, end_station_name, end_station_latitude AS lat, end_station_longitude AS long\n",
    "        FROM tripData\n",
    "        )\n",
    "    GROUP BY year, month, day, hour, end_station_name, lat, long\n",
    "    ORDER BY year, month, day, hour, end_station_name) AS arrival\n",
    "ON\n",
    "    departure.start_station_name = arrival.end_station_name AND\n",
    "    departure.year = arrival.year AND\n",
    "    departure.month = arrival.month AND\n",
    "    departure.day = arrival.day AND\n",
    "    departure.hour = arrival.hour\n",
    "\"\"\")\n",
    "aggregate.write.mode('overwrite').saveAsTable('aggregateTripData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_trip_weather = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT b.year, b.month, b.day, b.hour, station_name, lat, long, departure_count, arrival_count, wind_speed_rate, visibility, temperature, precipitation\n",
    "    FROM\n",
    "        (SELECT YEAR(timestamp) AS year,\n",
    "        MONTH(timestamp) AS month,\n",
    "        DAY(timestamp) AS day,\n",
    "        HOUR(timestamp) AS hour,\n",
    "        wind_speed_rate,\n",
    "        visibility,\n",
    "        temperature,\n",
    "        precipitation\n",
    "        FROM weatherData) AS a\n",
    "    FULL OUTER JOIN\n",
    "        aggregateTripData AS b\n",
    "    ON a.year = b.year\n",
    "    AND a.month = b.month\n",
    "    AND a.day = b.day\n",
    "    AND a.hour = b.hour\n",
    "    )\n",
    "WHERE month IS NOT NULL\n",
    "\"\"\")\n",
    "aggregate_trip_weather.createOrReplaceTempView('aggregateTripWeatherData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|incident_category                           |\n",
      "+--------------------------------------------+\n",
      "|Vehicle Misplaced                           |\n",
      "|Suspicious                                  |\n",
      "|Forgery And Counterfeiting                  |\n",
      "|Sex Offense                                 |\n",
      "|Family Offense                              |\n",
      "|Fire Report                                 |\n",
      "|Assault                                     |\n",
      "|Recovered Vehicle                           |\n",
      "|Drug Violation                              |\n",
      "|Robbery                                     |\n",
      "|Motor Vehicle Theft?                        |\n",
      "|Embezzlement                                |\n",
      "|Vehicle Impounded                           |\n",
      "|Missing Person                              |\n",
      "|Rape                                        |\n",
      "|Human Trafficking (B), Involuntary Servitude|\n",
      "|Human Trafficking, Commercial Sex Acts      |\n",
      "|Lost Property                               |\n",
      "|Arson                                       |\n",
      "|Fraud                                       |\n",
      "|Homicide                                    |\n",
      "|Drug Offense                                |\n",
      "|Weapons Offence                             |\n",
      "|Suicide                                     |\n",
      "|Other                                       |\n",
      "|Gambling                                    |\n",
      "|Burglary                                    |\n",
      "|Human Trafficking (A), Commercial Sex Acts  |\n",
      "|Warrant                                     |\n",
      "|Traffic Violation Arrest                    |\n",
      "|Courtesy Report                             |\n",
      "|Traffic Collision                           |\n",
      "|Weapons Carrying Etc                        |\n",
      "|Weapons Offense                             |\n",
      "|Other Offenses                              |\n",
      "|Miscellaneous Investigation                 |\n",
      "|Non-Criminal                                |\n",
      "|Prostitution                                |\n",
      "|Civil Sidewalks                             |\n",
      "|Case Closure                                |\n",
      "|Suspicious Occ                              |\n",
      "|Larceny Theft                               |\n",
      "|Other Miscellaneous                         |\n",
      "|Liquor Laws                                 |\n",
      "|Malicious Mischief                          |\n",
      "|Disorderly Conduct                          |\n",
      "|Juvenile Offenses                           |\n",
      "|Offences Against The Family And Children    |\n",
      "|Motor Vehicle Theft                         |\n",
      "|Vandalism                                   |\n",
      "|Stolen Property                             |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT incident_category FROM crimeData\n",
    "GROUP BY incident_category\n",
    "\"\"\").show(60, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|incident_category                           |\n",
      "+--------------------------------------------+\n",
      "|Suspicious                                  |\n",
      "|Sex Offense                                 |\n",
      "|Family Offense                              |\n",
      "|Assault                                     |\n",
      "|Drug Violation                              |\n",
      "|Robbery                                     |\n",
      "|Rape                                        |\n",
      "|Human Trafficking (B), Involuntary Servitude|\n",
      "|Human Trafficking, Commercial Sex Acts      |\n",
      "|Arson                                       |\n",
      "|Fraud                                       |\n",
      "|Homicide                                    |\n",
      "|Drug Offense                                |\n",
      "|Weapons Offence                             |\n",
      "|Burglary                                    |\n",
      "|Human Trafficking (A), Commercial Sex Acts  |\n",
      "|Traffic Violation Arrest                    |\n",
      "|Traffic Collision                           |\n",
      "|Weapons Carrying Etc                        |\n",
      "|Weapons Offense                             |\n",
      "|Prostitution                                |\n",
      "|Suspicious Occ                              |\n",
      "|Larceny Theft                               |\n",
      "|Liquor Laws                                 |\n",
      "|Malicious Mischief                          |\n",
      "|Disorderly Conduct                          |\n",
      "|Juvenile Offenses                           |\n",
      "|Offences Against The Family And Children    |\n",
      "|Motor Vehicle Theft                         |\n",
      "|Vandalism                                   |\n",
      "|Stolen Property                             |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "crime_data = spark.sql(\"\"\"\n",
    "SELECT * FROM crimeData\n",
    "WHERE incident_category != \"Vehicle Misplaced\"\n",
    "AND incident_category != \"Forgery And Counterfeiting\"\n",
    "AND incident_category != \"Fire Report\"\n",
    "AND incident_category != \"Recovered Vehicle\"\n",
    "AND incident_category != \"Vehicle Impounded\"\n",
    "AND incident_category != \"Embezzlement\"\n",
    "AND incident_category != \"Lost Property\"\n",
    "AND incident_category != \"Suicide\"\n",
    "AND incident_category != \"Other\"\n",
    "AND incident_category != \"Gambling\"\n",
    "AND incident_category != \"Warrant\"\n",
    "AND incident_category != \"Courtesy Report\"\n",
    "AND incident_category != \"Miscellaneous Investigation\"\n",
    "AND incident_category != \"Non-Criminal\"\n",
    "AND incident_category != \"Civil Sidewalks\"\n",
    "AND incident_category != \"Case Closure\"\n",
    "AND incident_category != \"Other Miscellaneous\"\n",
    "AND incident_category != \"Missing Person\"\n",
    "AND incident_category != \"Other Offenses\"\n",
    "\"\"\")\n",
    "crime_data = crime_data.withColumn('incident_category', F.when(crime_data['incident_category'] == 'Motor Vehicle Theft?', 'Motor Vehicle Theft').otherwise(crime_data['incident_category']))\n",
    "crime_data.createOrReplaceTempView('crimeData')\n",
    "spark.sql(\"\"\"\n",
    "SELECT incident_category FROM crimeData\n",
    "GROUP BY incident_category\n",
    "\"\"\").show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations = spark.sql(\"\"\"\n",
    "SELECT DISTINCT start_station_name AS name, start_station_latitude AS lat, start_station_longitude AS long\n",
    "FROM tripData\n",
    "\"\"\")\n",
    "bike_stations.createOrReplaceTempView('bikeStations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_to_station = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT name, incident_timestamp, incident_id, distance, DENSE_RANK() OVER (PARTITION BY incident_timestamp, incident_id ORDER BY distance) AS rank\n",
    "    FROM\n",
    "        (\n",
    "        SELECT a.name, b.incident_timestamp, b.incident_id, \n",
    "        acos(sin(radians(a.lat)) * sin(radians(b.lat)) +\n",
    "                cos(radians(a.lat)) * cos(radians(b.lat)) *\n",
    "                cos(radians(a.long - b.long))) * 6372.8 AS distance\n",
    "        FROM\n",
    "            bikeStations AS a\n",
    "        CROSS JOIN\n",
    "            crimeData AS b\n",
    "        )\n",
    "    )\n",
    "WHERE rank = 1\n",
    "\"\"\")\n",
    "crime_to_station.write.mode('overwrite').saveAsTable('crimeToStation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_tripWeatherCrime = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT year, month, day, hour, station_name, lat, long, departure_count, arrival_count, wind_speed_rate, visibility, temperature, precipitation, num_incidents\n",
    "    FROM\n",
    "        (SELECT COUNT(1) AS num_incidents, name, DATE(incident_timestamp) AS date\n",
    "        FROM crimeToStation\n",
    "        GROUP BY date, name) AS a\n",
    "    FULL OUTER JOIN\n",
    "        aggregateTripWeatherData AS b\n",
    "    ON CAST(CONCAT(year, \"-\", month, \"-\", day) AS date) = DATE_ADD(date, 1)\n",
    "    AND a.name = b.station_name\n",
    "    )\n",
    "WHERE month IS NOT NULL\n",
    "\"\"\")\n",
    "aggregate_tripWeatherCrime = aggregate_tripWeatherCrime.withColumn('num_incidents', F.when(aggregate_tripWeatherCrime['num_incidents'].isNull(), 0).otherwise(aggregate_tripWeatherCrime['num_incidents']))\n",
    "aggregate_tripWeatherCrime.coalesce(1).write.format('parquet').mode('overwrite').save('aggregateTripWeatherCrimeData.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 2055941 points in the final aggregated dataset\n"
     ]
    }
   ],
   "source": [
    "print(f'There are now {aggregate_tripWeatherCrime.count()} points in the final aggregated dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split the data into the training/validation/test sets and save as parquet files so we can easily access the sets during modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('resources/v2/v2_aggregate_trip_weather_crime_data.parquet').createOrReplaceTempView('v2data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "|year|month|day|hour|        station_name|departure_count|arrival_count|wind_speed_rate|visibility|temperature|precipitation|num_incidents|weekday|holiday|\n",
      "+----+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "|2018|    2|  8|  12|10th Ave at E 15t...|              0|            1|             15|     16093|       11.1|            0|            0|      1|      0|\n",
      "|2018|    5| 15|  21|10th Ave at E 15t...|              0|            1|             77|     16093|       18.9|            0|            0|      1|      0|\n",
      "|2018|    6|  4|   9|10th Ave at E 15t...|              1|            0|             77|     16093|       14.4|            0|            0|      1|      0|\n",
      "|2018|    8| 21|   8|10th Ave at E 15t...|              1|            0|             51|     16093|       13.3|            0|            0|      1|      0|\n",
      "|2018|    9| 29|  20|10th Ave at E 15t...|              0|            1|             57|     16093|       20.0|            0|            0|      0|      0|\n",
      "|2019|    2| 13|  12|10th Ave at E 15t...|              1|            0|             93|      4023|       10.0|           10|            0|      1|      0|\n",
      "|2019|    3| 14|   8|10th Ave at E 15t...|              0|            1|              0|     14484|        8.3|            0|            0|      1|      0|\n",
      "|2019|    3| 17|  18|10th Ave at E 15t...|              1|            0|             31|     14484|       13.3|            0|            0|      0|      0|\n",
      "|2019|    4|  7|  17|10th Ave at E 15t...|              1|            0|             26|     14484|       16.7|            0|            0|      0|      0|\n",
      "|2019|    5| 19|  23|10th St at Empire St|              0|            1|             62|     14484|       14.4|            0|            0|      0|      0|\n",
      "|2019|    9|  2|  13|10th St at Empire St|              0|            1|             62|     16093|       16.7|            0|            0|      1|      1|\n",
      "|2018|    2| 23|  13|10th St at Fallon St|              0|            1|             26|     16093|        5.6|            0|            0|      1|      0|\n",
      "|2018|    3| 18|  17|10th St at Fallon St|              0|            1|             15|     16093|       10.6|            0|            0|      0|      0|\n",
      "|2018|    3| 19|  11|10th St at Fallon St|              1|            0|             21|     16093|        7.8|            0|            0|      1|      0|\n",
      "|2018|    3| 27|  18|10th St at Fallon St|              2|            0|             15|     16093|       16.7|            0|            0|      1|      0|\n",
      "|2018|    4| 28|  10|10th St at Fallon St|              0|            2|             41|     16093|       13.3|            0|            0|      0|      0|\n",
      "|2018|    5|  9|  13|10th St at Fallon St|              0|            1|             82|     12875|       13.3|            0|            0|      1|      0|\n",
      "|2018|    6|  5|   6|10th St at Fallon St|              0|            1|            103|     16093|       13.9|            0|            0|      1|      0|\n",
      "|2018|    8|  2|  11|10th St at Fallon St|              0|            1|             67|     16093|       11.7|            0|            0|      1|      0|\n",
      "|2018|    8| 17|  16|10th St at Fallon St|              2|            0|              0|     14484|       15.6|            0|            0|      1|      0|\n",
      "+----+-----+---+----+--------------------+---------------+-------------+---------------+----------+-----------+-------------+-------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeData = spark.read.csv('resources/v2/v2_time.csv',\n",
    "                          header=True,\n",
    "                          inferSchema=True)\n",
    "timeData.createOrReplaceTempView('temp')\n",
    "spark.sql(\"\"\"\n",
    "SELECT TO_TIMESTAMP(date, \"yyyy/mm/dd\") AS date, weekday, holiday\n",
    "FROM temp\n",
    "\"\"\").createOrReplaceTempView('timeData')\n",
    "mlData = spark.sql(\"\"\"\n",
    "SELECT a.year, a.month, a.day, hour, station_name, departure_count, arrival_count, wind_speed_rate, visibility, temperature, precipitation, num_incidents, weekday, holiday\n",
    "FROM\n",
    "v2data AS a\n",
    "JOIN\n",
    "timeData AS b\n",
    "ON a.year == YEAR(b.date)\n",
    "AND a.month == MONTH(b.date)\n",
    "AND a.day == DAY(b.date)\n",
    "\"\"\")\n",
    "mlData.createOrReplaceTempView('v2data_withtime')\n",
    "mlData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM v2data_withtime\n",
    "WHERE (year = 2018 AND month = 1)\n",
    "OR (year = 2018 AND month = 2)\n",
    "OR (year = 2018 AND month = 3)\n",
    "OR (year = 2018 AND month = 4)\n",
    "OR (year = 2018 AND month = 5)\n",
    "OR (year = 2018 AND month = 6)\n",
    "OR (year = 2018 AND month = 7)\n",
    "OR (year = 2018 AND month = 8)\n",
    "OR (year = 2018 AND month = 9)\n",
    "OR (year = 2018 AND month = 10)\n",
    "OR (year = 2018 AND month = 11)\n",
    "OR (year = 2018 AND month = 12)\n",
    "OR (year = 2019 AND month = 1)\n",
    "OR (year = 2019 AND month = 2)\n",
    "OR (year = 2019 AND month = 3)\n",
    "OR (year = 2019 AND month = 4)\n",
    "OR (year = 2019 AND month = 5)\n",
    "OR (year = 2019 AND month = 6)\n",
    "OR (year = 2019 AND month = 7)\n",
    "\"\"\").coalesce(1).write.format('parquet').mode('overwrite').save('training.parquet')\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM v2data_withtime\n",
    "WHERE (year = 2019 AND month = 8)\n",
    "OR (year = 2019 AND month = 9)\n",
    "\"\"\").coalesce(1).write.format('parquet').mode('overwrite').save('validation.parquet')\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM v2data_withtime\n",
    "WHERE (year = 2019 AND month = 10)\n",
    "OR (year = 2019 AND month = 11)\n",
    "OR (year = 2019 AND month = 12)\n",
    "OR (year = 2020 AND month = 1)\n",
    "\"\"\").coalesce(1).write.format('parquet').mode('overwrite').save('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the datasets are prepared so we are ready to move onto the modelling step. First, we need to load all of the training/validation/testing sets. Any station we model needs to have data points present in all three sets. Also, since we are creating models for each station, we need to make sure to only model stations where we have at least 100 data points in the training set. Any station not matching these requirements will be given constant inventory levels; we can assume they are not very popular stations and a constant inventory level will be sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.parquet('resources/v2/training.parquet')\n",
    "training = training.dropna()\n",
    "validation = spark.read.parquet('resources/v2/validation.parquet')\n",
    "validation = validation.dropna()\n",
    "testing = spark.read.parquet('resources/v2/test.parquet')\n",
    "testing = testing.dropna()\n",
    "training_station_names = training.select('station_name').distinct().collect()\n",
    "valid_station_names = list(map(lambda x : x.__getattr__('station_name'),\n",
    "                               validation.select('station_name').distinct().collect()))\n",
    "test_station_names = list(map(lambda x : x.__getattr__('station_name'),\n",
    "                              testing.select('station_name').distinct().collect()))\n",
    "station_names = []\n",
    "for row in training_station_names:\n",
    "    rowname = row.__getattr__('station_name')\n",
    "    if rowname in valid_station_names and rowname in test_station_names:\n",
    "        if training.filter(training.station_name == rowname).count() >= 100:\n",
    "            station_names.append(rowname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression to predict departure counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "f = open('resources/validations/departure_linear_regression_validation.txt', 'w+')\n",
    "for rowname in station_names:\n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'departure_count').withColumnRenamed('departure_count',\n",
    "                                                                      'label')\n",
    "    lr = LinearRegression() \n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.regParam,\n",
    "                                           [0.3, 0.5, 0.7]).addGrid(lr.elasticNetParam,\n",
    "                                                                    [0.5, 0.8]).build()\n",
    "    cvModel = CrossValidator(estimator=lr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "             str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                     labelCol='departure_count',\n",
    "                                     metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/lr/departure/{rowname}',\n",
    "                                                      format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression to predict arrival counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "f = open('resources/validations/arrival_linear_regression_validation.txt', 'w+')\n",
    "for rowname in station_names: \n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'arrival_count').withColumnRenamed('arrival_count',\n",
    "                                                                    'label')\n",
    "    lr = LinearRegression() \n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.regParam,\n",
    "                                           [0.3, 0.5, 0.7]).addGrid(lr.elasticNetParam,\n",
    "                                                                    [0.5, 0.8]).build()\n",
    "    cvModel = CrossValidator(estimator=lr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "             str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                     labelCol='arrival_count',\n",
    "                                     metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/lr/arrival/{rowname}',\n",
    "                                                      format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosted forest to predict departure counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "f = open('resources/validations/departure_gradient_boosted_forest_validation.txt', 'w+')\n",
    "for rowname in station_names:\n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'departure_count').withColumnRenamed('departure_count',\n",
    "                                                                      'label')\n",
    "    gbtr = GBTRegressor()\n",
    "    paramGrid = ParamGridBuilder().addGrid(gbtr.maxDepth,\n",
    "                                           [3, 5]).build()\n",
    "    cvModel = CrossValidator(estimator=gbtr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "             str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                     labelCol='departure_count',\n",
    "                                     metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/gbtr/departure/{rowname}', format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosted forest to predict arrival counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "f = open('resources/validations/arrival_gradient_boosted_forest_validation.txt', 'w+')\n",
    "for rowname in station_names:\n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'arrival_count').withColumnRenamed('arrival_count',\n",
    "                                                                    'label')\n",
    "    gbtr = GBTRegressor()\n",
    "    paramGrid = ParamGridBuilder().addGrid(gbtr.maxDepth,\n",
    "                                           [3, 5]).build()\n",
    "    cvModel = CrossValidator(estimator=gbtr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "             str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                     labelCol='arrival_count',\n",
    "                                     metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/gbtr/arrival/{rowname}', format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest to predict departure counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "f = open('resources/validations/departure_random_forest_validation.txt', 'w+')\n",
    "for rowname in station_names:\n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'departure_count').withColumnRenamed('departure_count',\n",
    "                                                                      'label')\n",
    "    rfr = RandomForestRegressor()\n",
    "    paramGrid = ParamGridBuilder().addGrid(rfr.numTrees,\n",
    "                                           [50, 100]).build()\n",
    "    cvModel = CrossValidator(estimator=rfr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "            str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                    labelCol='departure_count',\n",
    "                                    metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/rfr/departure/{rowname}', format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest to predict arrival counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "f = open('resources/validations/arrival_random_forest_validation.txt', 'w+')\n",
    "for rowname in station_names:\n",
    "    print('.', end='')\n",
    "    filteredTraining = training.filter(training.station_name == rowname)\n",
    "    filteredValid = validation.filter(validation.station_name == rowname)\n",
    "    filteredTest = testing.filter(testing.station_name == rowname)\n",
    "    vectorA = VectorAssembler(inputCols=['month',\n",
    "                                         'day',\n",
    "                                         'hour',\n",
    "                                         'wind_speed_rate',\n",
    "                                         'visibility',\n",
    "                                         'temperature',\n",
    "                                         'precipitation',\n",
    "                                         'num_incidents',\n",
    "                                         'weekday',\n",
    "                                         'holiday'],\n",
    "                              outputCol='features')\n",
    "    stationDF = vectorA.transform(filteredTraining)\n",
    "    stationDF = stationDF.select('features',\n",
    "                                 'arrival_count').withColumnRenamed('arrival_count',\n",
    "                                                                    'label')\n",
    "    rfr = RandomForestRegressor()\n",
    "    paramGrid = ParamGridBuilder().addGrid(rfr.numTrees,\n",
    "                                           [50, 100]).build()\n",
    "    cvModel = CrossValidator(estimator=rfr,\n",
    "                             estimatorParamMaps=paramGrid,\n",
    "                             evaluator=RegressionEvaluator(),\n",
    "                             numFolds=3).fit(stationDF)\n",
    "    validDF = vectorA.transform(filteredValid)\n",
    "    validPred = cvModel.transform(validDF)\n",
    "    f.write('R^2 value of cvModel on validation set ' + \n",
    "            str(RegressionEvaluator(predictionCol='prediction',\n",
    "                                    labelCol='arrival_count',\n",
    "                                    metricName='r2').evaluate(validPred)) + '\\n')\n",
    "    testDF = vectorA.transform(filteredTest)\n",
    "    testPred = cvModel.transform(testDF)\n",
    "    testPred.coalesce(1).write.mode('overwrite').save(f'resources/rfr/arrival/{rowname}', format='json')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the $R^2$ values of the validation tests, we can see that gradient-boosted tree and random forest are both performing far better than linear regression. However, we can also see that for all three methods, there is quite a large range of $R^2$ values across all stations. This could be due to a not insignificant subset of station being  used on a sporadic basis. The extremely popular stations may have high correlations to the selected features, but less popular stations may be used more randomly. In V3, we will analyze these results, create visualizations using the data we have generated from these models, and pose hypotheses about the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] P. Hulot, D. Aloise, and S.D. Jena, \"Towards Station-Level Demand Prediction for Effective Rebalancing in Bike-Sharing Systems,\" In KDD'18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discover \\& Data Mining, 2018, pp. 378-386. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
