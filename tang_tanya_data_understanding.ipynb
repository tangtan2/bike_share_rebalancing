{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIE1512H Project\n",
    "\n",
    "### Topic: Predicting Station-Level Demand in Bike-Sharing Systems\n",
    "\n",
    "Name: Tanya Tang<br>\n",
    "Version : V1<br>\n",
    "Date: March 11, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the paper this project is based on is to improve the effectiveness of station-level rebalancing activites within bike-sharing systems by obtaining more accurate minimum inventory levels per station. \n",
    "\n",
    "The authors used historical bike trip data along with hourly weather data to train several machine learning models. The model features fell within two categories, time-related features from the bike data (i.e. year, month, etc.) and weather-related features (i.e. precipitation, temperature, etc.). To reduce overfitting, the data is first transformed onto a reduced space using four different reduction techniques:\n",
    "1. No reduction, baseline\n",
    "2. Group all stations into one cluster\n",
    "3. Kmeans - cluster stations into k clusters\n",
    "4. Singular value decomposition\n",
    "\n",
    "After reducing the problem, four different prediction methods were tested:\n",
    "1. Linear regression\n",
    "2. Multi-layer perceptron\n",
    "3. Gradient boosted tree\n",
    "4. Random forest\n",
    "\n",
    "Once a prediction has been made on the reduced problem, the reduction needs to be inverted to obtain the full problem predictions. Each reduction technique has its own specific inversion process. \n",
    "\n",
    "The solution techniques were tested and scored based on the performance of its generated minimum station inventory levels. \n",
    "\n",
    "In this project, a similar methodology will be followed, with modifications and additions on the feature construction aspect. Focusing on the city of San Francisco, historical bike data and weather data will be used in conjunction with crime data. Features from all three data sets will be constructed, and several prediction techniques will be tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Project Versions/Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Data Understanding (collecting, describing, exploring, verifying)\n",
    "* Data Preparation on Test Scale (selecting, cleaning, constructing, integrating, formatting)\n",
    "\n",
    "Timeline:\n",
    "* Looking up potential datasets and collecting from online sources/deciding which datasets to use (0.5 hr)\n",
    "* Exploring entries/schema of each chosen dataset to understand discrepancies and connections between datasets (1 hr)\n",
    "* Loading/cleaning raw data within each dataset, taking a closer look at which entries are relevant (2 hr)\n",
    "* Calculating connections between different datasets and joining tables to eventually form a single aggregated table containing all relevant information for a single entry (4 hrs)\n",
    "* Verifying data preparation steps (i.e. tables joined correctly, entries were not lost throughout the preparation process, no null values exist) (1 hr)\n",
    "\n",
    "#### V2\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Data Preparation on Full Scale (selecting, cleaning, constructing, integrating, formatting)\n",
    "* Modeling (selecting techniques, generating test design, building, assessing)\n",
    "\n",
    "Planned Timeline (Weeks 1 and 2: March 12 to 26):\n",
    "* Recreate data preparation tasks from V1 on full dataset (1 hr)\n",
    "* Research reduction/prediction techniques and how to apply them using python packages (1 hr)\n",
    "* Implement reduction techniques (4 hr)\n",
    "* Implement regression techniques (4 hr)\n",
    "* Implement inversion techniques (4 hr)\n",
    "* Test implemented models (1 hr)\n",
    "\n",
    "#### V3\n",
    "\n",
    "CRISP-DM Tasks:\n",
    "* Evaluation (evaluting results, reviewing process, determining next steps)\n",
    "\n",
    "Planned Timeline (Week 3: March 26 to April 2):\n",
    "* Analyze results and make hypotheses (3 hrs)\n",
    "* Review project activites to verify correctness (1 hr)\n",
    "* Summarize project and list some potential next steps (1 hr)\n",
    "\n",
    "#### F\n",
    "\n",
    "Planned Timeline (Week 4: April 2 to April 9):\n",
    "* Write report (6 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Data Understanding/Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of data from three datasets will be analyzed, cleaned, and linked in this notebook. These datasets include:\n",
    "* San Francisco Ford GoBike Share (bike trips)\n",
    "* NOAA Hourly Surface Data (weather)\n",
    "* SFPD Incident Reports (crime)\n",
    "\n",
    "All of these datasets will go towards predicting station-level demand prediction in bike-sharing systems. The aforementioned paper only uses bike trip and weather information to make predictions. However, they noted that oftentimes, crime rates are also correlated with bike share usage. Thus, due to the availability of incidents reports from the San Francisco Police Department, I will include an extra feature: the number of relevant crime incidents around a certain bike station per day. \n",
    "\n",
    "For the purposes of this version, I will only be working with data from January and February of 2018. The full data preparation will include data from January 2018 to February 2020. Following a similar partitioning as the paper, training will be done on data from January 2018 to July 2019, validation will be done on data from August to October 2019, and testing will be done on data from November 2019 to January 2020. \n",
    "\n",
    "**San Francisco Ford GoBike Share**\n",
    "1. Data is split into different csv files, partitioned by month. Each csv file has the same schema. There are no empty entries and no malformed data. \n",
    "2. Stations are distinguished by names along with latitude/longitude coordinates. Encoding is not necessary as a separate model needs to be created and solved for each station. \n",
    "\n",
    "**NOAA Hourly Surface Data**\n",
    "1. Data is split into different csv files, partitioned by year. Each csv file has the same schema. There are no empty entries under the relevant headings, but care needs to be taken to only include hourly reports of type FM-15. These hourly reports are also taken at minute 56 of each hour, so to simplify the data, four minutes are added to the timestamp so that each report timestamp refers to the end of the hour it has recorded data for. \n",
    "2. There are no categorical variables. \n",
    "\n",
    "**SFPD Incident Reports**\n",
    "1. All data from the beginning of 2018 is included in one csv file. There are no empty entries under the relevant headings. However, there are many incidents which refer to non-relevant activity (e.g. hidden crimes, white-collar crimes, non-criminal incidents). Thus, all irrelevant entries need to be purged from the dataset so the number of incidents will accurately reflect what people are observing in the neighbourhood. \n",
    "2. Incidents are categorized by the type of crime/activity, but we are not concerned with that level of granularity in the data and only care about the number of relevant criminal incidents close to each station per day. Thus, there is no need to encode any of the incident categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Spark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Java locations\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/\"\n",
    "os.environ[\"JRE_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/\"\n",
    "\n",
    "# Initialize spark\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/Cellar/apache-spark@2.3.2/2.3.2/libexec/\")\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"appName\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trip data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration_sec: integer (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- bike_share_for_all_trip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in ['01', '02']:\n",
    "    filepath = 'resources/2018' + i + '-fordgobike-tripdata.csv'\n",
    "    month_data = spark.read.csv(filepath,\n",
    "                               header=True,\n",
    "                               inferSchema=True,\n",
    "                               sep=',',\n",
    "                               mode='DROPMALFORMED')\n",
    "    data.append(month_data)\n",
    "for i in range(1, len(data)):\n",
    "    data[0] = data[0].union(data[i])\n",
    "data[0].createOrReplaceTempView('tripData')\n",
    "data[0].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load crime data and enforce schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- incident_timestamp: timestamp (nullable = true)\n",
      " |-- incident_id: integer (nullable = true)\n",
      " |-- report_code: string (nullable = true)\n",
      " |-- incident_code: integer (nullable = true)\n",
      " |-- incident_category: string (nullable = true)\n",
      " |-- incident_subcategory: string (nullable = true)\n",
      " |-- resolution: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'resources/sfpd_crime.csv'\n",
    "crime_data = spark.read.csv(filepath,\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           sep=',',\n",
    "                           mode='DROPMALFORMED')\n",
    "crime_data.createOrReplaceTempView('temp')\n",
    "crime_data = spark.sql(\"\"\"\n",
    "SELECT * FROM\n",
    "    (\n",
    "    SELECT TO_TIMESTAMP(`Incident Datetime`, \"yyyy/MM/dd hh:mm\") AS incident_timestamp,\n",
    "    `Incident ID` AS incident_id,\n",
    "    `Report Type Code` AS report_code,\n",
    "    `Incident Code` AS incident_code,\n",
    "    `Incident Category` AS incident_category,\n",
    "    `Incident Subcategory` AS incident_subcategory,\n",
    "    `Resolution` AS resolution,\n",
    "    `Latitude` as lat,\n",
    "    `Longitude` as long\n",
    "    FROM temp\n",
    "    WHERE `Latitude` IS NOT NULL\n",
    "    AND `Incident Category` IS NOT NULL\n",
    "    )\n",
    "WHERE MONTH(incident_timestamp) <= 2\n",
    "AND YEAR(incident_timestamp) = 2018\n",
    "\"\"\")\n",
    "crime_data.createOrReplaceTempView('crimeData')\n",
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weather data and enforce schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- wind_speed_rate: integer (nullable = true)\n",
      " |-- visibility: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- precipitation: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "filepath = 'resources/sf_weather.csv'\n",
    "weather_data = spark.read.csv(filepath,\n",
    "                              header=True,\n",
    "                              inferSchema=True,\n",
    "                              sep=',',\n",
    "                              mode='DROPMALFORMED')\n",
    "weather_data.createOrReplaceTempView('temp')\n",
    "weather_data = spark.sql(\"\"\"\n",
    "SELECT * FROM\n",
    "    (\n",
    "    SELECT (DATE + INTERVAL '4' MINUTE) AS timestamp,\n",
    "    LATITUDE AS lat,\n",
    "    LONGITUDE AS long,\n",
    "    WND AS wind_speed_rate,\n",
    "    VIS AS visibility,\n",
    "    TMP AS temperature,\n",
    "    AA1 AS precipitation\n",
    "    FROM temp\n",
    "    WHERE SOURCE = 7 AND REPORT_TYPE = \"FM-15\"\n",
    "    )\n",
    "WHERE MONTH(timestamp) <= 2\n",
    "\"\"\")\n",
    "weather_data = weather_data.withColumn('wind_speed_rate', split('wind_speed_rate', '\\,')[3])\n",
    "weather_data = weather_data.withColumn('wind_speed_rate', weather_data['wind_speed_rate'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('visibility', split('visibility', '\\,')[0])\n",
    "weather_data = weather_data.withColumn('visibility', weather_data['visibility'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('temperature', split('temperature', '\\,')[0])\n",
    "weather_data = weather_data.withColumn('temperature', weather_data['temperature'].cast(IntegerType()))\n",
    "weather_data = weather_data.withColumn('temperature', weather_data['temperature'] / 10)\n",
    "weather_data = weather_data.withColumn('precipitation', split('precipitation', '\\,')[1])\n",
    "weather_data = weather_data.withColumn('precipitation', weather_data['precipitation'].cast(IntegerType()))\n",
    "weather_data.createOrReplaceTempView('weatherData')\n",
    "weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate trip data and replace table with new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = spark.sql(\"\"\"\n",
    "SELECT\n",
    "COALESCE(departure_count, 0) AS departure_count,\n",
    "COALESCE(arrival_count, 0) AS arrival_count,\n",
    "COALESCE(departure.hour, arrival.hour) AS hour,\n",
    "COALESCE(departure.day, arrival.day) AS day,\n",
    "COALESCE(departure.month, arrival.month) AS month,\n",
    "COALESCE(departure.start_station_name, arrival.end_station_name) AS station_name,\n",
    "COALESCE(departure.lat, arrival.lat) AS lat,\n",
    "COALESCE(departure.long, arrival.long) AS long\n",
    "FROM\n",
    "    (SELECT COUNT(1) AS departure_count, hour, day, month, start_station_name, lat, long\n",
    "    FROM\n",
    "        (\n",
    "        SELECT MONTH(start_time) AS month, DAY(start_time) AS day, HOUR(start_time) AS hour, start_station_name, start_station_latitude AS lat, start_station_longitude AS long\n",
    "        FROM tripData\n",
    "        )\n",
    "    GROUP BY start_station_name, lat, long, hour, day, month\n",
    "    ORDER BY month, day, hour, start_station_name) AS departure\n",
    "FULL OUTER JOIN\n",
    "    (SELECT COUNT(1) AS arrival_count, hour, day, month, end_station_name, lat, long\n",
    "    FROM\n",
    "        (\n",
    "        SELECT MONTH(end_time) AS month, DAY(end_time) AS day, HOUR(end_time) AS hour, end_station_name, end_station_latitude AS lat, end_station_longitude AS long\n",
    "        FROM tripData\n",
    "        )\n",
    "    GROUP BY month, day, hour, end_station_name, lat, long\n",
    "    ORDER BY month, day, hour, end_station_name) AS arrival\n",
    "ON\n",
    "    departure.start_station_name = arrival.end_station_name AND\n",
    "    departure.month = arrival.month AND\n",
    "    departure.day = arrival.day AND\n",
    "    departure.hour = arrival.hour\n",
    "\"\"\")\n",
    "aggregate.write.mode('overwrite').saveAsTable('aggregateTripData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze weather data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|num_hours|day|month|\n",
      "+---------+---+-----+\n",
      "|       23|  1|    1|\n",
      "|       23| 31|    1|\n",
      "+---------+---+-----+\n",
      "\n",
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "|          timestamp|    lat|     long|wind_speed_rate|visibility|temperature|precipitation|\n",
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "|2018-01-01 01:00:00|37.6197|-122.3647|             15|     12875|       13.9|            0|\n",
      "|2018-01-01 02:00:00|37.6197|-122.3647|             21|     16093|       13.3|            0|\n",
      "|2018-01-01 03:00:00|37.6197|-122.3647|             21|     16093|       12.8|            0|\n",
      "|2018-01-01 04:00:00|37.6197|-122.3647|              0|     16093|       12.2|            0|\n",
      "|2018-01-01 05:00:00|37.6197|-122.3647|             15|     16093|       12.2|            0|\n",
      "|2018-01-01 06:00:00|37.6197|-122.3647|              0|     16093|       12.2|            0|\n",
      "|2018-01-01 07:00:00|37.6197|-122.3647|             15|     16093|       11.7|            0|\n",
      "|2018-01-01 08:00:00|37.6197|-122.3647|             26|     14484|       11.7|            0|\n",
      "|2018-01-01 09:00:00|37.6197|-122.3647|             21|     11265|       11.1|            0|\n",
      "|2018-01-01 10:00:00|37.6197|-122.3647|              0|     11265|       10.6|            0|\n",
      "|2018-01-01 11:00:00|37.6197|-122.3647|              0|     12875|        8.9|            0|\n",
      "|2018-01-01 12:00:00|37.6197|-122.3647|             21|     14484|        8.3|            0|\n",
      "|2018-01-01 13:00:00|37.6197|-122.3647|             15|     14484|        8.3|            0|\n",
      "|2018-01-01 14:00:00|37.6197|-122.3647|              0|     14484|        7.8|            0|\n",
      "|2018-01-01 15:00:00|37.6197|-122.3647|             15|      9656|        7.2|            0|\n",
      "|2018-01-01 16:00:00|37.6197|-122.3647|              0|      9656|        8.3|            0|\n",
      "|2018-01-01 17:00:00|37.6197|-122.3647|              0|      8047|       10.6|            0|\n",
      "|2018-01-01 18:00:00|37.6197|-122.3647|              0|      6437|       11.7|            0|\n",
      "|2018-01-01 19:00:00|37.6197|-122.3647|             21|      6437|       11.7|            0|\n",
      "|2018-01-01 20:00:00|37.6197|-122.3647|             15|      6437|       12.2|            0|\n",
      "|2018-01-01 21:00:00|37.6197|-122.3647|             15|      6437|       12.8|            0|\n",
      "|2018-01-01 22:00:00|37.6197|-122.3647|             21|     12875|       13.9|            0|\n",
      "|2018-01-01 23:00:00|37.6197|-122.3647|             21|     12875|       13.9|            0|\n",
      "|2018-01-31 00:00:00|37.6197|-122.3647|             15|     16093|       15.6|            0|\n",
      "|2018-01-31 01:00:00|37.6197|-122.3647|             41|     16093|       16.7|            0|\n",
      "|2018-01-31 02:00:00|37.6197|-122.3647|             51|     16093|       13.9|            0|\n",
      "|2018-01-31 03:00:00|37.6197|-122.3647|             57|     16093|       13.3|            0|\n",
      "|2018-01-31 04:00:00|37.6197|-122.3647|             57|     16093|       13.3|            0|\n",
      "|2018-01-31 05:00:00|37.6197|-122.3647|             36|     16093|       13.3|            0|\n",
      "|2018-01-31 06:00:00|37.6197|-122.3647|             41|     16093|       12.8|            0|\n",
      "|2018-01-31 07:00:00|37.6197|-122.3647|             41|     16093|       11.7|            0|\n",
      "|2018-01-31 09:00:00|37.6197|-122.3647|              0|     16093|       11.1|            0|\n",
      "|2018-01-31 10:00:00|37.6197|-122.3647|              0|     16093|       10.6|            0|\n",
      "|2018-01-31 11:00:00|37.6197|-122.3647|             26|     16093|        9.4|            0|\n",
      "|2018-01-31 12:00:00|37.6197|-122.3647|              0|     16093|        8.3|            0|\n",
      "|2018-01-31 13:00:00|37.6197|-122.3647|              0|     16093|        8.9|            0|\n",
      "|2018-01-31 14:00:00|37.6197|-122.3647|             21|     16093|        8.3|            0|\n",
      "|2018-01-31 15:00:00|37.6197|-122.3647|              0|     16093|        9.4|            0|\n",
      "|2018-01-31 16:00:00|37.6197|-122.3647|              0|     12875|       10.6|            0|\n",
      "|2018-01-31 17:00:00|37.6197|-122.3647|              0|      4828|       11.7|            0|\n",
      "|2018-01-31 18:00:00|37.6197|-122.3647|             21|     14484|       12.8|            0|\n",
      "|2018-01-31 19:00:00|37.6197|-122.3647|             15|     16093|       13.3|            0|\n",
      "|2018-01-31 20:00:00|37.6197|-122.3647|             21|     16093|       13.9|            0|\n",
      "|2018-01-31 21:00:00|37.6197|-122.3647|             21|     16093|       14.4|            0|\n",
      "|2018-01-31 22:00:00|37.6197|-122.3647|             21|     16093|       15.0|            0|\n",
      "|2018-01-31 23:00:00|37.6197|-122.3647|             51|     16093|       17.8|            0|\n",
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT COUNT(1) AS num_hours, DAY(timestamp) AS day, MONTH(timestamp) AS month\n",
    "    FROM weatherData\n",
    "    GROUP BY day, month\n",
    "    ORDER BY month, day\n",
    "    )\n",
    "WHERE num_hours < 24\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM weatherData\n",
    "WHERE MONTH(timestamp) = 1\n",
    "AND (DAY(timestamp) = 1 OR DAY(timestamp) = 31)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no entry for 00:00 on January 1, 2018 because the hourly data is in reference to the hour designated by its end time in *timestamp*, so we can just copy the data from 01:00 on January 1. But, it appears we are missing data for the hour 07:00 to 08:00 on January 31, 2018, so to compensate, we can pull data from the preceding and following entry to fill in that blank with an average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entry0 = spark.sql(\"\"\"\n",
    "SELECT '2018-01-01 00:00:00', lat, long, wind_speed_rate,\n",
    "visibility, temperature, precipitation\n",
    "FROM weatherData\n",
    "WHERE timestamp = '2018-01-01 01:00:00'\n",
    "\"\"\")\n",
    "new_entry1 = spark.sql(\"\"\"\n",
    "SELECT '2018-01-31 08:00:00', AVG(lat), AVG(long), AVG(wind_speed_rate),\n",
    "AVG(visibility), AVG(temperature), AVG(precipitation)\n",
    "FROM weatherData\n",
    "WHERE timestamp = '2018-01-31 07:00:00'\n",
    "OR timestamp = '2018-01-31 09:00:00'\n",
    "\"\"\")\n",
    "weather_data = weather_data.union(new_entry0)\n",
    "weather_data = weather_data.union(new_entry1)\n",
    "weather_data = weather_data.orderBy(['timestamp'])\n",
    "weather_data.createOrReplaceTempView('weatherData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "|          timestamp|    lat|     long|wind_speed_rate|visibility|temperature|precipitation|\n",
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "|2018-01-01 00:00:00|37.6197|-122.3647|           15.0|   12875.0|       13.9|          0.0|\n",
      "|2018-01-01 01:00:00|37.6197|-122.3647|           15.0|   12875.0|       13.9|          0.0|\n",
      "|2018-01-01 02:00:00|37.6197|-122.3647|           21.0|   16093.0|       13.3|          0.0|\n",
      "|2018-01-01 03:00:00|37.6197|-122.3647|           21.0|   16093.0|       12.8|          0.0|\n",
      "|2018-01-01 04:00:00|37.6197|-122.3647|            0.0|   16093.0|       12.2|          0.0|\n",
      "|2018-01-01 05:00:00|37.6197|-122.3647|           15.0|   16093.0|       12.2|          0.0|\n",
      "|2018-01-01 06:00:00|37.6197|-122.3647|            0.0|   16093.0|       12.2|          0.0|\n",
      "|2018-01-01 07:00:00|37.6197|-122.3647|           15.0|   16093.0|       11.7|          0.0|\n",
      "|2018-01-01 08:00:00|37.6197|-122.3647|           26.0|   14484.0|       11.7|          0.0|\n",
      "|2018-01-01 09:00:00|37.6197|-122.3647|           21.0|   11265.0|       11.1|          0.0|\n",
      "|2018-01-01 10:00:00|37.6197|-122.3647|            0.0|   11265.0|       10.6|          0.0|\n",
      "|2018-01-01 11:00:00|37.6197|-122.3647|            0.0|   12875.0|        8.9|          0.0|\n",
      "|2018-01-01 12:00:00|37.6197|-122.3647|           21.0|   14484.0|        8.3|          0.0|\n",
      "|2018-01-01 13:00:00|37.6197|-122.3647|           15.0|   14484.0|        8.3|          0.0|\n",
      "|2018-01-01 14:00:00|37.6197|-122.3647|            0.0|   14484.0|        7.8|          0.0|\n",
      "|2018-01-01 15:00:00|37.6197|-122.3647|           15.0|    9656.0|        7.2|          0.0|\n",
      "|2018-01-01 16:00:00|37.6197|-122.3647|            0.0|    9656.0|        8.3|          0.0|\n",
      "|2018-01-01 17:00:00|37.6197|-122.3647|            0.0|    8047.0|       10.6|          0.0|\n",
      "|2018-01-01 18:00:00|37.6197|-122.3647|            0.0|    6437.0|       11.7|          0.0|\n",
      "|2018-01-01 19:00:00|37.6197|-122.3647|           21.0|    6437.0|       11.7|          0.0|\n",
      "+-------------------+-------+---------+---------------+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM weatherData\n",
    "WHERE DAY(timestamp) = 31\n",
    "OR DAY(timestamp) = 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate clean weather data with trip data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_trip_weather = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT b.month, b.day, b.hour, station_name, lat, long, departure_count, arrival_count,\n",
    "    wind_speed_rate, visibility, temperature, precipitation\n",
    "    FROM\n",
    "        (SELECT MONTH(timestamp) AS month,\n",
    "        DAY(timestamp) AS day,\n",
    "        HOUR(timestamp) AS hour,\n",
    "        wind_speed_rate,\n",
    "        visibility,\n",
    "        temperature,\n",
    "        precipitation\n",
    "        FROM weatherData) AS a\n",
    "    FULL OUTER JOIN\n",
    "        aggregateTripData AS b\n",
    "    ON a.month = b.month\n",
    "    AND a.day = b.day\n",
    "    AND a.hour = b.hour\n",
    "    )\n",
    "WHERE month IS NOT NULL\n",
    "\"\"\")\n",
    "aggregate_trip_weather.createOrReplaceTempView('aggregateTripWeatherData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all stations are captured in start stations from trip data by ensuring the count of start and end stations are the same. This will allow us to just use the lat/long coordinates of the start stations to calculate which station each crime incident happens closest to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|start_count|\n",
      "+-----------+\n",
      "|        277|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|end_count|\n",
      "+---------+\n",
      "|      277|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as start_count\n",
    "FROM\n",
    "    (\n",
    "    SELECT DISTINCT start_station_name AS name, start_station_latitude AS lat, start_station_longitude AS long\n",
    "    FROM tripData\n",
    "    )\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as end_count\n",
    "FROM\n",
    "    (\n",
    "    SELECT DISTINCT end_station_name AS name, end_station_latitude AS lat, end_station_longitude AS long\n",
    "    FROM tripData\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categories an incident can be placed in, but there most likely exists some categories that will not impact human behaviour within a specific neighbourhood, such as accidental fires, suicides, white-collar crimes, etc. Incidents relating to these categories should be removed such that they do not skew the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|incident_category                         |\n",
      "+------------------------------------------+\n",
      "|Vehicle Misplaced                         |\n",
      "|Suspicious                                |\n",
      "|Forgery And Counterfeiting                |\n",
      "|Sex Offense                               |\n",
      "|Family Offense                            |\n",
      "|Fire Report                               |\n",
      "|Assault                                   |\n",
      "|Recovered Vehicle                         |\n",
      "|Drug Violation                            |\n",
      "|Robbery                                   |\n",
      "|Motor Vehicle Theft?                      |\n",
      "|Embezzlement                              |\n",
      "|Vehicle Impounded                         |\n",
      "|Missing Person                            |\n",
      "|Rape                                      |\n",
      "|Lost Property                             |\n",
      "|Arson                                     |\n",
      "|Fraud                                     |\n",
      "|Homicide                                  |\n",
      "|Drug Offense                              |\n",
      "|Suicide                                   |\n",
      "|Other                                     |\n",
      "|Gambling                                  |\n",
      "|Burglary                                  |\n",
      "|Human Trafficking (A), Commercial Sex Acts|\n",
      "|Warrant                                   |\n",
      "|Traffic Violation Arrest                  |\n",
      "|Courtesy Report                           |\n",
      "|Traffic Collision                         |\n",
      "|Weapons Carrying Etc                      |\n",
      "|Weapons Offense                           |\n",
      "|Other Offenses                            |\n",
      "|Miscellaneous Investigation               |\n",
      "|Non-Criminal                              |\n",
      "|Prostitution                              |\n",
      "|Civil Sidewalks                           |\n",
      "|Case Closure                              |\n",
      "|Suspicious Occ                            |\n",
      "|Larceny Theft                             |\n",
      "|Other Miscellaneous                       |\n",
      "|Liquor Laws                               |\n",
      "|Disorderly Conduct                        |\n",
      "|Malicious Mischief                        |\n",
      "|Offences Against The Family And Children  |\n",
      "|Motor Vehicle Theft                       |\n",
      "|Vandalism                                 |\n",
      "|Stolen Property                           |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT incident_category FROM crimeData\n",
    "GROUP BY incident_category\n",
    "\"\"\").show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be several categories that will not impact visible crime within an area. These categories are:\n",
    "* Vehicle Misplaced\n",
    "* Forgery and Counterfeiting\n",
    "* Fire Report\n",
    "* Recovered Vehicle\n",
    "* Motor Vehicle Theft? (this appears to be a mistake, aggregate with Motor Vehicle Theft)\n",
    "* Vehicle Impounded\n",
    "* Embezzlement\n",
    "* Lost Property\n",
    "* Suicide\n",
    "* Other\n",
    "* Gambling\n",
    "* Warrant\n",
    "* Courtesy Report\n",
    "* Missing Person\n",
    "* Miscellaneous Investigation\n",
    "* Non-Criminal\n",
    "* Civil Sidewalks\n",
    "* Case Closure\n",
    "* Other Miscellaneous\n",
    "* Other Offenses\n",
    "\n",
    "Incidents relating to these categories should be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|incident_category                         |\n",
      "+------------------------------------------+\n",
      "|Suspicious                                |\n",
      "|Sex Offense                               |\n",
      "|Family Offense                            |\n",
      "|Assault                                   |\n",
      "|Drug Violation                            |\n",
      "|Robbery                                   |\n",
      "|Rape                                      |\n",
      "|Arson                                     |\n",
      "|Fraud                                     |\n",
      "|Homicide                                  |\n",
      "|Drug Offense                              |\n",
      "|Burglary                                  |\n",
      "|Human Trafficking (A), Commercial Sex Acts|\n",
      "|Traffic Violation Arrest                  |\n",
      "|Traffic Collision                         |\n",
      "|Weapons Carrying Etc                      |\n",
      "|Weapons Offense                           |\n",
      "|Prostitution                              |\n",
      "|Suspicious Occ                            |\n",
      "|Larceny Theft                             |\n",
      "|Liquor Laws                               |\n",
      "|Disorderly Conduct                        |\n",
      "|Malicious Mischief                        |\n",
      "|Offences Against The Family And Children  |\n",
      "|Motor Vehicle Theft                       |\n",
      "|Vandalism                                 |\n",
      "|Stolen Property                           |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "crime_data = spark.sql(\"\"\"\n",
    "SELECT * FROM crimeData\n",
    "WHERE incident_category != \"Vehicle Misplaced\"\n",
    "AND incident_category != \"Forgery And Counterfeiting\"\n",
    "AND incident_category != \"Fire Report\"\n",
    "AND incident_category != \"Recovered Vehicle\"\n",
    "AND incident_category != \"Vehicle Impounded\"\n",
    "AND incident_category != \"Embezzlement\"\n",
    "AND incident_category != \"Lost Property\"\n",
    "AND incident_category != \"Suicide\"\n",
    "AND incident_category != \"Other\"\n",
    "AND incident_category != \"Gambling\"\n",
    "AND incident_category != \"Warrant\"\n",
    "AND incident_category != \"Courtesy Report\"\n",
    "AND incident_category != \"Miscellaneous Investigation\"\n",
    "AND incident_category != \"Non-Criminal\"\n",
    "AND incident_category != \"Civil Sidewalks\"\n",
    "AND incident_category != \"Case Closure\"\n",
    "AND incident_category != \"Other Miscellaneous\"\n",
    "AND incident_category != \"Missing Person\"\n",
    "AND incident_category != \"Other Offenses\"\n",
    "\"\"\")\n",
    "crime_data = crime_data.withColumn('incident_category', F.when(crime_data['incident_category'] == 'Motor Vehicle Theft?', 'Motor Vehicle Theft').otherwise(crime_data['incident_category']))\n",
    "crime_data.createOrReplaceTempView('crimeData')\n",
    "spark.sql(\"\"\"\n",
    "SELECT incident_category FROM crimeData\n",
    "GROUP BY incident_category\n",
    "\"\"\").show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign each crime incident to its nearest bike station using lat/long coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_stations = spark.sql(\"\"\"\n",
    "SELECT DISTINCT start_station_name AS name, start_station_latitude AS lat, start_station_longitude AS long\n",
    "FROM tripData\n",
    "\"\"\")\n",
    "bike_stations.createOrReplaceTempView('bikeStations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_to_station = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT name, incident_timestamp, incident_id, distance, DENSE_RANK() OVER (PARTITION BY incident_timestamp, incident_id ORDER BY distance) AS rank\n",
    "    FROM\n",
    "        (\n",
    "        SELECT a.name, b.incident_timestamp, b.incident_id, \n",
    "        acos(sin(radians(a.lat)) * sin(radians(b.lat)) +\n",
    "                cos(radians(a.lat)) * cos(radians(b.lat)) *\n",
    "                cos(radians(a.long - b.long))) * 6372.8 AS distance\n",
    "        FROM\n",
    "            bikeStations AS a\n",
    "        CROSS JOIN\n",
    "            crimeData AS b\n",
    "        )\n",
    "    )\n",
    "WHERE rank = 1\n",
    "\"\"\")\n",
    "crime_to_station.write.mode('overwrite').saveAsTable('crimeToStation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure each crime incident has been assigned a station by comparing the size of the two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569 16569\n"
     ]
    }
   ],
   "source": [
    "print(crime_to_station.count(), crime_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate crime incidents per day and join with aggregated trip/weather data. It is better to use the previous day's crime rate than the daily crime incidents as a feature because the daily rate is unknown during that day. If there is no entry in crime incident table, then there were 0 incidents on that day for that station, so all the null values in *num_incidents* can be replaced with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_tripWeatherCrime = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "    (\n",
    "    SELECT month, day, hour, station_name, lat, long, departure_count, arrival_count, wind_speed_rate, visibility, temperature, precipitation, num_incidents\n",
    "    FROM\n",
    "        (SELECT COUNT(1) AS num_incidents, name, DATE(incident_timestamp) AS date\n",
    "        FROM crimeToStation\n",
    "        GROUP BY date, name) AS a\n",
    "    FULL OUTER JOIN\n",
    "        aggregateTripWeatherData AS b\n",
    "    ON CAST(CONCAT(\"2018-\", month, \"-\", day) AS date) = DATE_ADD(date, 1)\n",
    "    AND a.name = b.station_name\n",
    "    )\n",
    "WHERE month IS NOT NULL\n",
    "\"\"\")\n",
    "aggregate_tripWeatherCrime = aggregate_tripWeatherCrime.withColumn('num_incidents', F.when(aggregate_tripWeatherCrime['num_incidents'].isNull(), 0).otherwise(aggregate_tripWeatherCrime['num_incidents']))\n",
    "aggregate_tripWeatherCrime.coalesce(1).write.format('parquet').mode('overwrite').save('aggregateTripWeatherCrimeData.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the table **aggregateTripWeatherCrimeData** is cleaned and structured. All relevant information have been included and linked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] P. Hulot, D. Aloise, and S.D. Jena, \"Towards Station-Level Demand Prediction for Effective Rebalancing in Bike-Sharing Systems,\" In KDD'18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discover \\& Data Mining, 2018, pp. 378-386. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
